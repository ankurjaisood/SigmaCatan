Settling the Odds: AI Agents in Catan
=====================================
AA228/CS238 Final Project (Autumn 2024)
---------------------------------------

**SigmaCatan** is a research project aimed at developing an AI agent capable of playing *Settlers of Catan* using Deep Q-Networks (DQN). 
The goal is to improve upon simple random agents by navigating the large state-action space of Catan and learning strategic decision-making through reinforcement learning. 
This project uses a forked version of the Catanatron environment to generate training data and evaluate the agent’s performance.

## Features

- **DQN-based Training**: Implements a Deep Q-Learning pipeline with a neural network architecture and experience replay.
- **Offline RL Setup**: Uses pre-collected data from Catanatron for training without relying on online exploration.
- **Custom Reward Functions**: Experiments with different reward functions to encourage both immediate and long-term strategic actions.
- **Static Board Configuration**: Simplifies the problem by training on a fixed Catan board layout.
- **Parameterization Simplifications**: Avoids complex player-to-player trading and parameterized actions to improve training stability.

## Project Structure

- `agents/`: Contains agent implementations and utility scripts.
- `datasets/`: Directory for pre-collected datasets (e.g., `board.json`, `data.json`) in JSON format.
               These datasets were generated by via the custom accumulators in our forked [Catanatron repo](https://github.com/cheskett/catanatron).
- `environment/`: Definitions of the various game components as well as the state and action space.
- `interfaces/`: Containts interfaces to parse datasets into our definition of the environment.
                 Currently `CatanatronParser` can be used to parse the pre-collected datasets located in `datasets/`.
- `models/`: Saved PyTorch models and checkpoints.
- `rewards/`: Containts reward functions which can be used for training.
- `dqn_player.py`: Plugin for Catanatron that allows the user to test models in the Catanatron environment.
- `main.py`: Launches model training.
- `run_experiments.sh`: Define various hyperparamater configurations and train them in order, saving the resulting model.
- `run_validation.sh`: Run validation tests of a model in the Catanatron environment following different action fallback policies.
- `README.md`: Project overview and instructions (this file).

## Setup Instructions

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/ankurjaisood/SigmaCatan.git
   cd SigmaCatan
   ```
2. **Install Dependencies**:
   We recommend using a virtual environment
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```
4. **Set Up Catanatron**:
   Our modified Catanatron environment can be found [Catanatron repo](https://github.com/cheskett/catanatron).
   Follow its README for build and run instructions.
   Make sure catanatron-play is in your PATH or adjust scripts accordingly.

## Usage
1. **Generate Data**:
   Use the Catanatron environment to simulate games and store results:
  ```bash
  catanatron-play --num=1000 --output=./data/ --json --players=AB:2:T,AB:2:T,AB:2:T,AB:2:T --config-map=TOURNAMENT
  ```
  This produces a dataset with a board.json and data.json for each game.
  The existing dataset `datasets/2024-12-04_10_31_09` can also be used which was generated from the above command.

2. **Training the DQN Agent**:
   Train the DQN agent using the pre-collected dataset and your choice of hyperparamater configuration.

  ```bash
  python main.py \
    --reward_func END_TURN_PENALTY \
    --gamma 0.9 \
    --num_epochs 20 \
    --target_update_freq 5000 \
    --loss_func huber \
    --end_turn_penalty 5 \
    --tau 0.001
  ```
  Adjust hyperparameters as needed.
  
  Alternatively, `run_experiments.sh` can be used to train a series of models with varying hyperparameter configurations.
  Update the paramater table with your desired hyperparameter configurations and run the script to run them sequentiallu.
  ```bash
  .
  .
  .
  # Parameter table:
  # Each line defines: reward_func gamma num_epochs target_update_freq loss_func end_turn_penalty tau
  params=(
      "BASIC 0.90 1 10000 huber 2.5 0.001"
  )
  .
  .
  .
```
  

3. **Evaluate the trained model(s):**
  Once a model has been generated, update `MODEL_PATH` in `dqn_player.py` and run the following command to test it in the Catanatron environment against random agents.

  ```bash
    catanatron-play --num=500 --code=dqn_player.py --players=DQN,R,R,R --config-map=TOURNAMENT
  ```

  The following parameters in `dqn_player.py` control the choice of action fallback policy that is applied as described in our report.
  ```python
  MASK_INVALID_ACTIONS = False
  DISALLOW_MODEL_END_TURN = False
  DISALLOW_MODEL_END_TURN_AND_SELECT_NEXT = False
  ```

  To test the various combination of action fallback policies pain free you can set `ENABLE_MULTI_ITER_CACHE` in `dqn_player.py` to `TRUE` and run `run_validation.sh`

## Results
- Best-performing model achieved up to a 48% win rate against three random agents (vs. 25% expected for a random policy).
- Reward shaping significantly impacted performance and agent behavior.
- Hyperparameter tuning (e.g., adjusting tau, gamma, TUF) led to stable convergence and improved results.

## Future Work
- Extend the agent to handle dynamic board configurations.
- Integrate more complex parameterized actions and player-to-player trades.
- Explore advanced model architectures (RNNs, Transformers).
- Implement online learning and self-play to improve the policy further.

For more details and background, see the project’s final report and documented code comments.
